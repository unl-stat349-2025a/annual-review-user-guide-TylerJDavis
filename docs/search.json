[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "User Guide: Implementation of Imputation Likelihood Methods in Estimating Missing Data Values in R Using the Mice Package",
    "section": "",
    "text": "Introduction\nData has become the lifeblood of today’s world. It is used in almost every industry to track, organize, and solve businesses problems. This is why it is more crucial than ever to understand how to handle data properly to get results that can help you grow your business. One of the issues that comes across when analyzing a data set in dealing with missing values. Our first instinct is to throw those values out and analyze that data that we do have, yet this is not a viable option according to Little, Little (2021). Little discusses in his paper a variety of ways to properly estimate missing values in a data set, one option is imputation likelihood methods. This method allows you to get estimates for your missing values which intern allows you to get accurate information from your data set as a whole. A package called “mice” found in R can complete these missing data estimations which allow you to properly analyze your data, Van Buuren and Groothuis-Oudshoorn (n.d.). This guide will attempt to discuss the processes and methods involved with using this package on a data set. This example will be used as a guide to understanding how to implement this package on future data that you may come across.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "guide.html",
    "href": "guide.html",
    "title": "Explanation of Example",
    "section": "",
    "text": "Motivation\nThe following example from the mice package will show the basics of how and when to use this package to estimate missing values in a data set. This will allow you to get more accurate information from your models which can lead to different conclusions. How you treat missing data in your analysis is key to getting accurate estimates of your coefficients, which can be the difference in what variables to include in your model. This intern can lead to different decisions being made based on incorrect coefficients or incorrect estimates.",
    "crumbs": [
      "Explanation of Example"
    ]
  },
  {
    "objectID": "guide.html#explanation-of-example",
    "href": "guide.html#explanation-of-example",
    "title": "1  Introduction",
    "section": "2.1 Explanation of Example",
    "text": "2.1 Explanation of Example\n\n2.1.1 Motivation\nThe example from the mice package will show the basics of how and when to use this package to estimate missing values in a data set. This will allow you to have a basis of understanding regarding estimation of missing values using the mice package.\n\n\n2.1.2 Example\n\nThe first step of this example is to load in the mice package. The next object is to load in an example data set from the mice package for demonstration. In most cases your data set will be loaded in before you call you package. This will allow you to see the structure of the data and if the mice package is necessary.\n\n\n# Load required package\nlibrary(mice)\n\nWarning: package 'mice' was built under R version 4.3.3\n\n\nWarning in check_dep_version(): ABI version mismatch: \nlme4 was built with Matrix ABI version 1\nCurrent Matrix ABI version is 0\nPlease re-install lme4 from source or restore original 'Matrix' package\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n# Load example dataset\ndata(nhanes2)\nhead(nhanes2)\n\n    age  bmi  hyp chl\n1 20-39   NA &lt;NA&gt;  NA\n2 40-59 22.7   no 187\n3 20-39   NA   no 187\n4 60-99   NA &lt;NA&gt;  NA\n5 20-39 20.4   no 113\n6 60-99   NA &lt;NA&gt; 184\n\n# Check the structure of the dataset\nstr(nhanes2)\n\n'data.frame':   25 obs. of  4 variables:\n $ age: Factor w/ 3 levels \"20-39\",\"40-59\",..: 1 2 1 3 1 3 1 1 2 2 ...\n $ bmi: num  NA 22.7 NA NA 20.4 NA 22.5 30.1 22 NA ...\n $ hyp: Factor w/ 2 levels \"no\",\"yes\": NA 1 1 NA 1 NA 1 1 1 NA ...\n $ chl: num  NA 187 187 NA 113 184 118 187 238 NA ...\n\n\nWe can see from the output provided we can see that type of variables we are dealing with along with what they will be coded as in the data set. We can see that we have 25 observations with 4 variables. We can also see the type of each variable which will come in handy later when selecting the imputation method to estimate the missing values.\n\nThe next step is optional and it is to create a plot that will allow you to see where the missing values are in your data set.\n\n# Check missing data pattern\nmd.pattern(nhanes2)\n\n\n\n\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27\n\n\n\nThis graph shows us the pattern of missingness between variables. This plot can be broken down into different parts, the bottom row tells you how many missing values are in each variable. The red boxes show missing values while blue boxes show non - missing values. The left hand column shows the number of rows that follow the missingness pattern, while the right hand column shows the number of missing values in each row. This graph gives us a visual description of the data and the mssingness pattern throughout. You can make this graph to deepen your understanding of the missingness patterns in your data which paint a better picture than just the structure of the data.\n\nYou now understand the missingness pattern in the data you must now apply methods of estimations for the mice package. You can do this by understanding how each variable is categorized. We can look back at the structure of the data found in step 1. This will allow us to see how to apply methods of estimation as it is based on the type of variables you are dealing with.\n\n\nThis chart allows you to see how to select imputation methods based on what type of variables you have. It is not necessary to understand deeply how these imputation methods work to run and execute the code flawlessly. Yet more information will be provided later in the paper via the vignette for this package. In our case we have four variables: Age, bmi, hyp, chl. We know that age is a factor with 3 levels so it is a multilevel categorical variable. We also know that bmi and chl are numerical. Finally we know from step two that hyp is a factor with 2 levels and is intern binary data. Described in the chart above.\nYou will now create the vector for our methods section.\n\n# Define the imputation method for each variable\nmeth &lt;- c(\"polyreg\", \"pmm\", \"logreg\", \"pmm\")  # logreg for binary, polyreg for categorical\n\nThis will now allow you to properly estimate your missing values based on each imputation method that you have chosen.\n\nThe next step in this process is to understand what our prediction matrix is and how to do it assemble it.\n\n# Define the predictor matrix (set diagonal to 0)\npred_matrix &lt;- quickpred(nhanes2)\npred_matrix\n\n    age bmi hyp chl\nage   0   0   0   0\nbmi   1   0   1   1\nhyp   1   0   0   1\nchl   1   1   1   0\n\n\nThe prediction matrix is a necessary part of the imputation calculation it tells us what values to use in the estimation process. The first row is aobu the variable age, if you remeber from the missingness graph in step 2, we know that age has no missing values and there for has all zeros in the first row. The second row of the matrix is describing bmi, we can see that we have a 1 in the columns of age, hyp, and chl. This means for the variable bmi we have missing values and the package will use age, hyp, and chl to estimate the missing values of bmi. This same process takes place for the remaining two columns.\nOur next step is finally to complete our imputation of the data.\n\n# Perform multiple imputation\nimp &lt;- mice(nhanes2, method = meth, predictorMatrix = pred_matrix, m = 5, maxit = 10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  1   2  bmi  hyp  chl\n  1   3  bmi  hyp  chl\n  1   4  bmi  hyp  chl\n  1   5  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  2   2  bmi  hyp  chl\n  2   3  bmi  hyp  chl\n  2   4  bmi  hyp  chl\n  2   5  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  3   2  bmi  hyp  chl\n  3   3  bmi  hyp  chl\n  3   4  bmi  hyp  chl\n  3   5  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  4   2  bmi  hyp  chl\n  4   3  bmi  hyp  chl\n  4   4  bmi  hyp  chl\n  4   5  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  5   2  bmi  hyp  chl\n  5   3  bmi  hyp  chl\n  5   4  bmi  hyp  chl\n  5   5  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  6   2  bmi  hyp  chl\n  6   3  bmi  hyp  chl\n  6   4  bmi  hyp  chl\n  6   5  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  7   2  bmi  hyp  chl\n  7   3  bmi  hyp  chl\n  7   4  bmi  hyp  chl\n  7   5  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  8   2  bmi  hyp  chl\n  8   3  bmi  hyp  chl\n  8   4  bmi  hyp  chl\n  8   5  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  9   2  bmi  hyp  chl\n  9   3  bmi  hyp  chl\n  9   4  bmi  hyp  chl\n  9   5  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n  10   2  bmi  hyp  chl\n  10   3  bmi  hyp  chl\n  10   4  bmi  hyp  chl\n  10   5  bmi  hyp  chl\n\n\n\nThe function mice will run the imputation and create the basis for the best estimation with your data. You should have a good understanding of every part of this function besides the m and maxit sections. The m section is the number of imputed data sets that were created, 5 being a base number. The maxit section is the number of iterations were used for the imputation algorithm. These values are both base values and can be changed depending on the situation.\n\nThe next step is to summarize the imputation data and compare the imputed data with the original data set.\n\n# Check summary of imputed data\nsummary(imp)\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n     age      bmi      hyp      chl \n      \"\"    \"pmm\" \"logreg\"    \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   0   0   0\nbmi   1   0   1   1\nhyp   1   0   0   1\nchl   1   1   1   0\n\n# Compare original vs. imputed data\nstripplot(imp, pch = 20, cex = 1.2)\n\n\n\n\n\n\n\n\n\nWhen you run this code you see how many imputed data sets you have. In other words how many estimations you have for the missing values. This is a harder topic to understand but in the code above it was specified to impute 5 data sets which means we get five estimates for each missing value. This allows us to pool this information in linear regression which will be show later. We also see the summary of the methods used for each variable. In the output we also get a graphs that compares the values from the numerical variables that were imputed, bmi and chl. This graph has for columns, 0 is the original data and 1 - 5 are each imputed data set. The blue dots are the original data and the pink dots are the imputed estimates. This graphs allows us to see how the estimates compare to the original data. In our case we can see that the imputed values fit very will with the original data and we do not see anything that does not look realistic.\n\nThe final step in our example is to fit a linear model with the data and pool across all imputed data sets to get a result. You can then compare the results of the pooled data vs the original data without missing values and see how you estimates would be different.\n\n# Analyze each imputed dataset and pool results (Example: Regression model)\nfit &lt;- with(imp, lm(bmi ~ age + hyp + chl))\npooled &lt;- pool(fit)\nsummary(pooled)\n\n         term    estimate  std.error  statistic        df     p.value\n1 (Intercept) 17.71300591 4.03979136  4.3846338  7.244195 0.002962482\n2    age40-59 -4.86856873 2.12294560 -2.2933083  7.408268 0.053529440\n3    age60-99 -7.05676367 2.34031436 -3.0153059  8.340125 0.015899235\n4      hypyes  1.52404300 1.93371697  0.7881417 15.309227 0.442647204\n5         chl  0.06085648 0.02299606  2.6463875  6.553650 0.035175917\n\n\n\nThe function with combines your imputed data with the linear model example above. The pool function then pools across all imputed data sets and gives you an average of all of your estimates, in our case 5. You then have a linear model “pool” that you can summarize and compare with our original data set with out imputation and missing values.\n\nclean_data &lt;- na.omit(nhanes2)\nfit1 &lt;- lm(bmi ~ age + hyp + chl, clean_data)\n\nsummary(fit1)\n\n\nCall:\nlm(formula = bmi ~ age + hyp + chl, data = clean_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2752 -1.5329  0.0223  1.7676  4.2609 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  14.96300    4.26799   3.506  0.00801 **\nage40-59     -6.61839    2.30156  -2.876  0.02065 * \nage60-99    -11.18114    3.36170  -3.326  0.01045 * \nhypyes        2.36161    2.53536   0.931  0.37886   \nchl           0.07954    0.02445   3.253  0.01164 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.234 on 8 degrees of freedom\nMultiple R-squared:  0.6697,    Adjusted R-squared:  0.5046 \nF-statistic: 4.055 on 4 and 8 DF,  p-value: 0.04379\n\n\nFrom the two summary statements above you can see that you have a vast difference in the estimates of the coefficients between fit1 without the NA values and pooled with estimates for missing values. This allows you to understand that estimating for missing values does have a major impact on the model and in the end the conclusions that you make.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "guide.html#conclusion",
    "href": "guide.html#conclusion",
    "title": "Explanation of Example",
    "section": "Conclusion",
    "text": "Conclusion\nThis user guide hopefully provides a good overall explanation of a basic analysis with the mice package. This explanation discussed the elements of the mice package and how to use them. This example was fairly basic for more detailed examples and other questions regarding the package consult the R documentation in the references page, Buuren and Groothuis-Oudshoorn (2011). If you are interested in the mathematics and proofs behind imputation and the specifies of why to use it in missing value situation consult Afifi and Elashoff (1966), Anderson (1957), and Little (2021)). These resources will provide you the information you need to handle any challenge regarding imputation methods and the mice package. Hopefully when analyzing data you now not only understand the importance of missing value estimation but know how to execute it with the mice package, in R.\n\n\n\n\nAfifi, A. A., and R. M. Elashoff. 1966. “Missing Observations in Multivariate Statistics: I. Review of the Literature.” Journal of the American Statistical Association 61 (315): 595–604. https://doi.org/10.2307/2282773.\n\n\nAnderson, T. W. 1957. “Maximum Likelihood Estimates for a Multivariate Normal Distribution When Some Observations Are Missing.” Journal of the American Statistical Association 52 (278): 200–203. https://doi.org/10.2307/2280845.\n\n\nBuuren, Stef Van, and Karin Groothuis-Oudshoorn. 2011. “Mice : Multivariate Imputation by Chained Equations in R.” Journal of Statistical Software 45 (3). https://doi.org/10.18637/jss.v045.i03.\n\n\nLittle, Roderick J. 2021. “Missing Data Assumptions.” Annual Review of Statistics and Its Application 8 (1): 89–107. https://doi.org/10.1146/annurev-statistics-040720-031104.",
    "crumbs": [
      "Explanation of Example"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Afifi, A. A., and R. M. Elashoff. 1966. “Missing Observations in\nMultivariate Statistics: I. Review of the Literature.”\nJournal of the American Statistical Association 61 (315):\n595–604. https://doi.org/10.2307/2282773.\n\n\nAnderson, T. W. 1957. “Maximum Likelihood Estimates for a\nMultivariate Normal Distribution When Some Observations Are\nMissing.” Journal of the American Statistical\nAssociation 52 (278): 200–203. https://doi.org/10.2307/2280845.\n\n\nBuuren, Stef Van, and Karin Groothuis-Oudshoorn. 2011.\n“Mice : Multivariate Imputation by\nChained Equations in R.” Journal of\nStatistical Software 45 (3). https://doi.org/10.18637/jss.v045.i03.\n\n\nLittle, Roderick J. 2021. “Missing Data Assumptions.”\nAnnual Review of Statistics and Its Application 8 (1): 89–107.\nhttps://doi.org/10.1146/annurev-statistics-040720-031104.\n\n\nVan Buuren, Stef, and Karin Groothuis-Oudshoorn. n.d. “Mice:\nMultivariate Imputation by Chained Equations.” https://doi.org/10.32614/CRAN.package.mice.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "topic.html",
    "href": "topic.html",
    "title": "Appendix A — Topic",
    "section": "",
    "text": "Pick some subset of the information covered in your annual review article that you are interested in exploring in more detail.\nIdentify the scope of your user guide - what will you cover? Specific software packages? When to use or not to use a specific technique?\nI will look into using likelihood methods for analyzing missing data, I will cover how to use it and when to use it. I will attempt to discover how to implement it in R. This will be done by showing a example of using the mice package, found in R. This example will show the basics and give a broad understanding of how to apply this package to a variety of issues.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Topic</span>"
    ]
  },
  {
    "objectID": "needs.html",
    "href": "needs.html",
    "title": "Appendix B — Needs Assessment",
    "section": "",
    "text": "What does someone trying to accomplish your chosen task need help with?\n\nThey will need help understanding why we need to estimate for missing values in our data set along with a method to use to properly estimate missing values. This will also mean they have to understand how and when to use the mice package in R and what methods to use for each variable.\n\nWhat parts are likely to be tricky?\n\nUnderstanding the methods mathematically can be difficult as I do not fully understand the methods but implementing them in R is a task that can definitely be understood. This can be done with the mice package. I think a tricky element of this package can be understanding the prediction matrix along with what it is actually doing when the imputation is taking place.\n\nWhat resources are already available on this topic that may be helpful? Look for e.g. software vignettes, package documentation, papers about software packages, and so on.\n\nThere is vignettes available which will be attached in a section in the user guide along with the references. We can see that there is also R documentation papers on this package which will also be attached.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Needs Assessment</span>"
    ]
  },
  {
    "objectID": "task-analysis.html",
    "href": "task-analysis.html",
    "title": "Appendix C — Task Analysis",
    "section": "",
    "text": "C.1 Additional Guidance\nYour check-in should answer these basic questions (and similar concerns that apply more directly to your topic).\nOnce you’ve completed the check-in, you can use this section to jump-start an introduction/set-up/getting started section in your user guide. This document should remain as an appendix to your main report.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Task Analysis</span>"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "User Guide: Implementation of Imputation Likelihood Methods in Estimating Missing Data Values in R Using the Mice Package",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe prerequisites to understanding how to use this package are basic statistical knowledge and ability on par with running a linear model in R. You have to understand how to recognize missing data and how to classify variables, numerical, categorical, etc. You need to know what missing values in a data set are. You must understand the basics of matrices along with applying different manipulations to data. The software needed to complete this task can be found in the R packages lattice and mice for the simple example which will be described in detail below. There is other packages that the you may need but that will be on a case by case basis. The pure imputation is done purely by the mice package itself which can be found in the example section.\n\n\n\n\nLittle, Roderick J. 2021. “Missing Data Assumptions.” Annual Review of Statistics and Its Application 8 (1): 89–107. https://doi.org/10.1146/annurev-statistics-040720-031104.\n\n\nVan Buuren, Stef, and Karin Groothuis-Oudshoorn. n.d. “Mice: Multivariate Imputation by Chained Equations.” https://doi.org/10.32614/CRAN.package.mice.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "guide.html#example",
    "href": "guide.html#example",
    "title": "Explanation of Example",
    "section": "Example",
    "text": "Example\n\nStep 1\nThe first step of this example is to load in the mice package into R, I am using R studio. The next object is to load in an example data set from the mice package for demonstration. In most cases your data set will be loaded in before you call you package. This will allow you to see the structure of the data and if the mice package is necessary. Packages for likelihood imputation can be found in a variety of languages not just R, that is just the language of the example.\n\nLoading in Data\n\n# Load required package\nlibrary(mice)\n\nWarning: package 'mice' was built under R version 4.3.3\n\n\nWarning in check_dep_version(): ABI version mismatch: \nlme4 was built with Matrix ABI version 1\nCurrent Matrix ABI version is 0\nPlease re-install lme4 from source or restore original 'Matrix' package\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n# Load example dataset\ndata(nhanes2)\nhead(nhanes2)\n\n    age  bmi  hyp chl\n1 20-39   NA &lt;NA&gt;  NA\n2 40-59 22.7   no 187\n3 20-39   NA   no 187\n4 60-99   NA &lt;NA&gt;  NA\n5 20-39 20.4   no 113\n6 60-99   NA &lt;NA&gt; 184\n\n# Check the structure of the dataset\nstr(nhanes2)\n\n'data.frame':   25 obs. of  4 variables:\n $ age: Factor w/ 3 levels \"20-39\",\"40-59\",..: 1 2 1 3 1 3 1 1 2 2 ...\n $ bmi: num  NA 22.7 NA NA 20.4 NA 22.5 30.1 22 NA ...\n $ hyp: Factor w/ 2 levels \"no\",\"yes\": NA 1 1 NA 1 NA 1 1 1 NA ...\n $ chl: num  NA 187 187 NA 113 184 118 187 238 NA ...\n\n\nWe can see from the structure of the data, the type of variables we are dealing with along with what they will be coded as in the data set. We can see that we have 25 observations with 4 variables. We can also see the type, binary; numerical; etc, of each variable, which will come in handy later when selecting the imputation method to estimate the missing values.\n\n\n\nStep 2\nThe next step is optional and it is to create a plot that will allow you to see where the missing values are in your data set.\n\nVisualizing Missing values in Data\n\n# Check missing data pattern\nmd.pattern(nhanes2)\n\n\n\n\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27\n\n\nThis graph shows us the pattern of missing values between variables. This plot can be broken down into different parts, the bottom row tells you how many missing values are in each variable. The red boxes show missing values while blue boxes show non - missing values. The right hand column shows the number of rows that follow the missing value pattern, while the left hand column shows the number of missing values in each row. This graph gives us a visual description of the data and the pattern throughout. You can make this graph to deepen your understanding of the missing value patterns in your data which paint a better picture than just the structure of the data.\n\n\n\nStep 3\nYou now understand the missing value pattern in the data, the next step is that you must now apply methods of estimations for the mice package. You can do this by understanding how each variable is categorized. We can look back at the structure of the data found in step 1. This will allow us to see how to apply methods of estimation as it is based on the type of variables you are dealing with.\n\nThis chart allows you to see how to select imputation methods based on what type of variables you have. It is not necessary to understand deeply how these imputation methods work to run and execute the code flawlessly. Yet more information will be provided later in the paper via the vignette for this package. In our case we have four variables: “age”, “bmi”, “hyp”, “chl”. We know that “age” is a factor with 3 levels so it is a multilevel categorical variable. We also know that “bmi” and “chl” are numerical. Finally we know from step two that “hyp” is a factor with 2 levels and is intern binary data. Described in the chart above.\nYou will now create the vector for our methods section.\n\nDefining Imputation Methods\n\n# Define the imputation method for each variable\nmeth &lt;- c(\"polyreg\", \"pmm\", \"logreg\", \"pmm\")  # logreg for binary, polyreg for categorical\n\n\n\nVariables and Imputation Methods\n\n“age” = “ployreg”\n“bmi” = “pmm”\n“hyp” = “logreg”\n“chl” = “pmm”\n\nThis will now allow you to properly estimate your missing values based on each imputation method that you have chosen.\n\n\n\nStep 4\nThe next step in this process is to understand what our prediction matrix is and how to do it assemble it.\n\nCreating Prediction Matrix\n\n# Define the predictor matrix (set diagonal to 0)\npred_matrix &lt;- quickpred(nhanes2)\npred_matrix\n\n    age bmi hyp chl\nage   0   0   0   0\nbmi   1   0   1   1\nhyp   1   0   0   1\nchl   1   1   1   0\n\n\nThe prediction matrix is a necessary part of the imputation calculation, as it tells us what values to use in the estimation process. The first row is about the variable age, if you remember from the missing value graph in step 2, we know that age has no missing values and there for has all zeros in the first row. The second row of the matrix is describing “bmi”, we can see that we have a 1 in the columns of “age”, “hyp”, and “chl”. This means for the variable “bmi” we have missing values and the package will use “age”, “hyp”, and “chl” to estimate the missing values of “bmi”. This same process takes place for the remaining two columns.\n\n\n\nStep 5\nOur next step is finally to complete our imputation of the data.\n\nRunning Imputation\n\n# Perform multiple imputation\nimp &lt;- mice(nhanes2, method = meth, predictorMatrix = pred_matrix, m = 5, maxit = 10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  1   2  bmi  hyp  chl\n  1   3  bmi  hyp  chl\n  1   4  bmi  hyp  chl\n  1   5  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  2   2  bmi  hyp  chl\n  2   3  bmi  hyp  chl\n  2   4  bmi  hyp  chl\n  2   5  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  3   2  bmi  hyp  chl\n  3   3  bmi  hyp  chl\n  3   4  bmi  hyp  chl\n  3   5  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  4   2  bmi  hyp  chl\n  4   3  bmi  hyp  chl\n  4   4  bmi  hyp  chl\n  4   5  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  5   2  bmi  hyp  chl\n  5   3  bmi  hyp  chl\n  5   4  bmi  hyp  chl\n  5   5  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  6   2  bmi  hyp  chl\n  6   3  bmi  hyp  chl\n  6   4  bmi  hyp  chl\n  6   5  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  7   2  bmi  hyp  chl\n  7   3  bmi  hyp  chl\n  7   4  bmi  hyp  chl\n  7   5  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  8   2  bmi  hyp  chl\n  8   3  bmi  hyp  chl\n  8   4  bmi  hyp  chl\n  8   5  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  9   2  bmi  hyp  chl\n  9   3  bmi  hyp  chl\n  9   4  bmi  hyp  chl\n  9   5  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n  10   2  bmi  hyp  chl\n  10   3  bmi  hyp  chl\n  10   4  bmi  hyp  chl\n  10   5  bmi  hyp  chl\n\n\nThe function mice will run the imputation and create the basis for the best estimation with your data. You should have a good understanding of every part of this function besides the m and maxit sections. The m section is the number of imputed data sets that were created, 5 being a base number. The maxit section is the number of iterations were used for the imputation algorithm. These values are both base values and can be changed depending on the situation.\n\n\n\nStep 6\nThe next step is to summarize the imputation data and compare the imputed data with the original data set.\n\nSummary of Imputation and Comparison Between Original\n\n# Check summary of imputed data\nsummary(imp)\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n     age      bmi      hyp      chl \n      \"\"    \"pmm\" \"logreg\"    \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   0   0   0\nbmi   1   0   1   1\nhyp   1   0   0   1\nchl   1   1   1   0\n\n# Compare original vs. imputed data\nstripplot(imp, pch = 20, cex = 1.2)\n\n\n\n\n\n\n\n\nWhen you run this code you see how many imputed data sets you have. In other words how many estimations you have for the missing values. This is a harder topic to understand but in the code above it was specified to impute 5 data sets, the m section, which means we get five estimates for each missing value. This allows us to pool this information in linear regression which will be show later. We also see the summary of the methods used for each variable. In the output we also get a graphs that compares the values from the numerical variables that were imputed, “bmi” and “chl”. This graph has six columns, 0 is the original data and 1 - 5 are each imputed data set. The blue dots are the original data and the pink dots are the imputed estimates. This graphs allows us to see how the estimates compare to the original data. In our case we can see that the imputed values fit very will with the original data and we do not see anything that does not look realistic.\n\n\n\nStep 7\nThe final step in our example is to fit a linear model with the data and pool across all imputed data sets to get a result. You can then compare the results of the pooled data vs the original data without missing values and see how you estimates would be different.\n\nPooling Imputed Data\n\n# Analyze each imputed dataset and pool results (Example: Regression model)\nfit &lt;- with(imp, lm(bmi ~ age + hyp + chl))\npooled &lt;- pool(fit)\nsummary(pooled)\n\n         term    estimate std.error  statistic        df      p.value\n1 (Intercept) 19.81383736 4.0708416  4.8672582 11.434623 0.0004443664\n2    age40-59 -4.91983268 2.4993465 -1.9684477  6.757262 0.0911758847\n3    age60-99 -6.73633160 2.5555176 -2.6359950  9.461797 0.0260170163\n4      hypyes  1.53628754 2.4728534  0.6212611  7.747339 0.5522636040\n5         chl  0.04915485 0.0219849  2.2358461 11.524233 0.0459985463\n\n\nThe function with combines your imputed data with the linear model example above. The pool function then pools across all imputed data sets and gives you an average of all of your estimates, in our case 5. You then have a linear model “pooled” that you can summarize and compare with our original data set with out imputation and missing values.\n\n\nOriginal Data in Linear Model\n\nclean_data &lt;- na.omit(nhanes2)\nfit1 &lt;- lm(bmi ~ age + hyp + chl, clean_data)\n\nsummary(fit1)\n\n\nCall:\nlm(formula = bmi ~ age + hyp + chl, data = clean_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2752 -1.5329  0.0223  1.7676  4.2609 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  14.96300    4.26799   3.506  0.00801 **\nage40-59     -6.61839    2.30156  -2.876  0.02065 * \nage60-99    -11.18114    3.36170  -3.326  0.01045 * \nhypyes        2.36161    2.53536   0.931  0.37886   \nchl           0.07954    0.02445   3.253  0.01164 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.234 on 8 degrees of freedom\nMultiple R-squared:  0.6697,    Adjusted R-squared:  0.5046 \nF-statistic: 4.055 on 4 and 8 DF,  p-value: 0.04379\n\n\nFrom the two summary statements above you can see that you have a vast difference in the estimates of the coefficients between fit1 without the NA values and pooled with estimates for missing values. This allows you to understand that estimating for missing values does have a major impact on the model and in the end the conclusions that you make.",
    "crumbs": [
      "Explanation of Example"
    ]
  }
]